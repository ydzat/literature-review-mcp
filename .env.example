# ============================================
# ArXiv MCP Server - 环境变量配置示例
# ============================================
# 复制此文件为 .env 并填入你的配置
# cp .env.example .env

# ============================================
# LLM Provider 配置（必需）
# ============================================

# Provider 类型：siliconflow, openai, custom
LLM_PROVIDER=siliconflow

# API Key（必需）
LLM_API_KEY=your_api_key_here

# API Base URL（可选，custom provider 时必需）
# LLM_BASE_URL=https://api.siliconflow.cn/v1

# 模型名称（可选，不设置则使用 provider 默认模型）
# LLM_MODEL=Qwen/Qwen2.5-7B-Instruct

# 最大输出 tokens（可选，不设置则自动从模型信息获取）
# LLM_MAX_TOKENS=4096

# 温度参数（可选，默认 0.7）
# LLM_TEMPERATURE=0.3


# ============================================
# Provider 配置示例
# ============================================

# --- SiliconFlow（默认）---
# LLM_PROVIDER=siliconflow
# LLM_API_KEY=your_siliconflow_key
# LLM_MODEL=Qwen/Qwen2.5-7B-Instruct
# 获取 API Key: https://cloud.siliconflow.cn/i/TxUlXG3u

# --- OpenAI ---
# LLM_PROVIDER=openai
# LLM_API_KEY=your_openai_key
# LLM_MODEL=gpt-4o
# 支持模型: gpt-4o, gpt-4o-mini, gpt-4-turbo, gpt-3.5-turbo

# --- Deepseek ---
# LLM_PROVIDER=custom
# LLM_BASE_URL=https://api.deepseek.com/v1
# LLM_API_KEY=your_deepseek_key
# LLM_MODEL=deepseek-chat
# 支持模型: deepseek-chat, deepseek-reasoner

# --- 其他 OpenAI 兼容 API ---
# LLM_PROVIDER=custom
# LLM_BASE_URL=https://your-api-endpoint/v1
# LLM_API_KEY=your_api_key
# LLM_MODEL=your_model_name


# ============================================
# 已知模型信息（自动识别）
# ============================================
# 以下模型会自动识别上下文窗口和最大输出 tokens：
#
# Qwen 系列:
#   - Qwen/Qwen2.5-7B-Instruct (32K context, 4K output)
#   - Qwen/Qwen2.5-72B-Instruct (128K context, 8K output)
#
# GPT 系列:
#   - gpt-4o (128K context, 16K output)
#   - gpt-4o-mini (128K context, 16K output)
#   - gpt-4-turbo (128K context, 4K output)
#   - gpt-3.5-turbo (16K context, 4K output)
#
# Claude 系列:
#   - claude-3-5-sonnet-20241022 (200K context, 8K output)
#   - claude-3-opus-20240229 (200K context, 4K output)
#   - claude-3-sonnet-20240229 (200K context, 4K output)
#   - claude-3-haiku-20240307 (200K context, 4K output)
#
# Deepseek 系列:
#   - deepseek-chat (128K context, 8K output)
#   - deepseek-reasoner (128K context, 8K output)
#
# 如果你的模型不在列表中，系统会使用默认值（128K context, 4K output）


# ============================================
# 智能压缩功能说明
# ============================================
# 本工具内置智能文本压缩系统，可自动处理超长论文：
#
# 1. 精确 Token 计算：使用 tiktoken 库精确计算 token 数
# 2. 章节识别：自动识别论文结构（Abstract, Method, Conclusion 等）
# 3. 分级压缩：根据章节重要性智能压缩
#    - Abstract/Method: 100% 保留
#    - Introduction/Conclusion: 90% 保留
#    - Experiment/Result: 80% 保留
#    - Discussion: 60% 保留
#    - Related Work: 50% 保留
#    - Appendix: 30% 保留
#    - Reference: 0% 保留（完全丢弃）
# 4. 滚动压缩：逐步合并，避免一次性处理超长文本
# 5. 语义压缩：调用 LLM 进行智能压缩，而非简单截断
#
# 示例：138K tokens 的超长论文 → 压缩到 38K tokens（压缩率 72.3%）


# ============================================
# 兼容性说明
# ============================================
# 旧版环境变量仍然支持（会自动映射）：
# SILICONFLOW_API_KEY → LLM_API_KEY
#
# 但建议使用新的 LLM_* 系列环境变量以获得更好的灵活性


# ============================================
# 数据存储
# ============================================
# 所有数据自动存储在 ~/.arxiv-mcp/ 目录：
# - arxiv-mcp.db - SQLite 数据库
# - pdfs/ - 下载的 PDF 文件
# - texts/ - 提取的文本内容
# - generated/ - 生成的文件（综述、微信文章等）
# - config.json - 运行时配置


# ============================================
# 调试模式（可选）
# ============================================
# 设置为 true 启用详细日志
DEBUG=false