/**
 * LLM Provider - ç»Ÿä¸€çš„ LLM è°ƒç”¨æ¥å£
 * æ”¯æŒ SiliconFlowã€OpenAI åŠå…¶ä»–å…¼å®¹ OpenAI API çš„æœåŠ¡
 */

import axios, { AxiosInstance } from 'axios';
import { LLMConfig, LLMRequest, LLMResponse, ModelInfo, KNOWN_MODELS, PROVIDER_DEFAULTS } from './types.js';
import { countTokens, identifySections, rollingCompression } from './smart-compression.js';

export class LLMProvider {
  private config: LLMConfig;
  private client: AxiosInstance;
  private modelInfo: ModelInfo | null = null;

  constructor(config: LLMConfig) {
    this.config = config;
    
    // è®¾ç½®é»˜è®¤ baseUrl
    if (!this.config.baseUrl) {
      const defaults = PROVIDER_DEFAULTS[this.config.provider];
      if (defaults) {
        this.config.baseUrl = defaults.baseUrl;
      } else {
        throw new Error(`æœªçŸ¥çš„ provider: ${this.config.provider}ï¼Œè¯·æä¾› baseUrl`);
      }
    }
    
    // åˆ›å»º axios å®¢æˆ·ç«¯
    this.client = axios.create({
      baseURL: this.config.baseUrl,
      headers: {
        'Authorization': `Bearer ${this.config.apiKey}`,
        'Content-Type': 'application/json'
      },
      timeout: 180000  // 3 åˆ†é’Ÿè¶…æ—¶
    });
    
    // è·å–æ¨¡å‹ä¿¡æ¯
    this.modelInfo = this.getModelInfo();
  }

  /**
   * è·å–æ¨¡å‹ä¿¡æ¯
   */
  private getModelInfo(): ModelInfo | null {
    // ä¼˜å…ˆä½¿ç”¨å·²çŸ¥æ¨¡å‹ä¿¡æ¯
    if (KNOWN_MODELS[this.config.model]) {
      return KNOWN_MODELS[this.config.model];
    }
    
    // å¦‚æœæ˜¯æœªçŸ¥æ¨¡å‹ï¼Œè¿”å›ä¿å®ˆçš„é»˜è®¤å€¼
    console.warn(`âš ï¸  æœªçŸ¥æ¨¡å‹: ${this.config.model}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®`);
    return {
      name: this.config.model,
      maxContextTokens: 32768,  // ä¿å®ˆä¼°è®¡
      maxOutputTokens: 4096,    // ä¿å®ˆä¼°è®¡
    };
  }

  /**
   * è·å–æ¨¡å‹çš„æœ€å¤§è¾“å‡º tokens
   */
  public getMaxOutputTokens(): number {
    // ä¼˜å…ˆä½¿ç”¨ç”¨æˆ·é…ç½®
    if (this.config.maxTokens) {
      return this.config.maxTokens;
    }
    
    // ä½¿ç”¨æ¨¡å‹ä¿¡æ¯
    if (this.modelInfo) {
      return this.modelInfo.maxOutputTokens;
    }
    
    // é»˜è®¤å€¼
    return 4096;
  }

  /**
   * è·å–æ¨¡å‹çš„æœ€å¤§ä¸Šä¸‹æ–‡ tokens
   */
  public getMaxContextTokens(): number {
    return this.modelInfo?.maxContextTokens || 32768;
  }

  /**
   * è°ƒç”¨ LLM
   */
  public async chat(request: LLMRequest): Promise<LLMResponse> {
    try {
      const maxTokens = request.maxTokens || this.getMaxOutputTokens();
      
      // æ„å»ºè¯·æ±‚ä½“
      const requestBody = {
        model: this.config.model,
        messages: request.messages,
        temperature: request.temperature ?? this.config.temperature ?? 0.7,
        max_tokens: maxTokens,
        stream: request.stream ?? false
      };
      
      // è°ƒç”¨ API
      const response = await this.client.post('/chat/completions', requestBody);
      
      // è§£æå“åº”
      const content = response.data.choices[0].message.content;
      const usage = response.data.usage ? {
        promptTokens: response.data.usage.prompt_tokens,
        completionTokens: response.data.usage.completion_tokens,
        totalTokens: response.data.usage.total_tokens
      } : undefined;
      
      return { content, usage };
      
    } catch (error: any) {
      // è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
      const errorDetail = error.response?.data 
        ? JSON.stringify(error.response.data) 
        : error.message;
      throw new Error(`LLM è°ƒç”¨å¤±è´¥: ${errorDetail}`);
    }
  }

  /**
   * ç®€åŒ–çš„èŠå¤©æ–¹æ³•
   * æ¥å— prompt å’Œå¯é€‰çš„ systemPromptï¼Œè‡ªåŠ¨æ„å»º messages
   *
   * @param prompt ç”¨æˆ·æç¤º
   * @param systemPrompt ç³»ç»Ÿæç¤ºï¼ˆå¯é€‰ï¼‰
   * @param options é€‰é¡¹
   * @returns LLM å“åº”å†…å®¹
   */
  public async simpleChat(
    prompt: string,
    systemPrompt?: string,
    options?: { temperature?: number; maxTokens?: number }
  ): Promise<string> {
    const messages: Array<{ role: 'system' | 'user' | 'assistant', content: string }> = [];

    if (systemPrompt) {
      messages.push({ role: "system", content: systemPrompt });
    }
    messages.push({ role: "user", content: prompt });

    const response = await this.chat({
      messages,
      temperature: options?.temperature,
      maxTokens: options?.maxTokens
    });

    return response.content;
  }

  /**
   * å¸¦æ™ºèƒ½å‹ç¼©çš„èŠå¤©æ–¹æ³•
   * è‡ªåŠ¨æ£€æµ‹è¾“å…¥é•¿åº¦ï¼Œè¶…é•¿æ—¶ä½¿ç”¨æ™ºèƒ½å‹ç¼©
   *
   * @param prompt ç”¨æˆ·æç¤º
   * @param systemPrompt ç³»ç»Ÿæç¤ºï¼ˆå¯é€‰ï¼‰
   * @param options é€‰é¡¹
   * @returns LLM å“åº”å†…å®¹
   */
  public async chatWithCompression(
    prompt: string,
    systemPrompt?: string,
    options?: {
      temperature?: number;
      maxTokens?: number;
      enableCompression?: boolean;  // æ˜¯å¦å¯ç”¨å‹ç¼©ï¼ˆé»˜è®¤ trueï¼‰
    }
  ): Promise<string> {
    try {
      // 1. è®¡ç®— token æ•°
      const systemTokens = systemPrompt ? countTokens(systemPrompt) : 0;
      const promptTokens = countTokens(prompt);
      const totalInputTokens = systemTokens + promptTokens;

      // 2. è·å–æ¨¡å‹é™åˆ¶
      const maxOutputTokens = this.getMaxOutputTokens();
      const maxContextTokens = this.getMaxContextTokens();
      const availableTokens = maxContextTokens - maxOutputTokens - 1000; // ç•™ 1000 tokens ç¼“å†²

      console.log(`ğŸ“Š Token ç»Ÿè®¡: ç³»ç»Ÿæç¤º ${systemTokens}, ç”¨æˆ·æç¤º ${promptTokens}, æ€»è®¡ ${totalInputTokens} / ${availableTokens}`);

      // 3. å¦‚æœè¶…é•¿ä¸”å¯ç”¨å‹ç¼©ï¼Œä½¿ç”¨æ™ºèƒ½å‹ç¼©
      let processedPrompt = prompt;
      const enableCompression = options?.enableCompression !== false;  // é»˜è®¤å¯ç”¨

      if (totalInputTokens > availableTokens && enableCompression) {
        console.log(`âš ï¸  è¾“å…¥è¶…é•¿ (${totalInputTokens} > ${availableTokens})ï¼Œå¯åŠ¨æ™ºèƒ½å‹ç¼©...`);

        // è¯†åˆ«ç« èŠ‚
        const sections = identifySections(prompt);
        console.log(`ğŸ“‘ è¯†åˆ«åˆ° ${sections.length} ä¸ªç« èŠ‚`);

        // æ»šåŠ¨å‹ç¼©
        processedPrompt = await rollingCompression(sections, this, availableTokens - systemTokens);

        const compressedTokens = countTokens(processedPrompt);
        const compressionRatio = ((1 - compressedTokens / promptTokens) * 100).toFixed(1);
        console.log(`âœ… å‹ç¼©å®Œæˆ: ${promptTokens} â†’ ${compressedTokens} tokens (å‹ç¼©ç‡: ${compressionRatio}%)`);
      } else if (totalInputTokens > availableTokens) {
        console.warn(`âš ï¸  è¾“å…¥è¶…é•¿ä½†å‹ç¼©å·²ç¦ç”¨ï¼Œå¯èƒ½å¯¼è‡´ API è°ƒç”¨å¤±è´¥`);
      }

      // 4. è°ƒç”¨ LLM
      return await this.simpleChat(processedPrompt, systemPrompt, {
        temperature: options?.temperature,
        maxTokens: options?.maxTokens
      });

    } catch (error) {
      console.error("è°ƒç”¨ LLM æ—¶å‡ºé”™:", error);
      throw new Error(`AI è°ƒç”¨å¤±è´¥: ${error instanceof Error ? error.message : String(error)}`);
    }
  }

  /**
   * ç²¾ç¡®è®¡ç®—æ–‡æœ¬çš„ token æ•°é‡
   * ä½¿ç”¨ tiktoken åº“è¿›è¡Œç²¾ç¡®è®¡ç®—
   *
   * @param text æ–‡æœ¬å†…å®¹
   * @returns token æ•°é‡
   */
  public countTokens(text: string): number {
    return countTokens(text, this.config.model);
  }

  /**
   * ä¼°ç®—æ–‡æœ¬çš„ token æ•°é‡ï¼ˆç²—ç•¥ä¼°è®¡ï¼‰
   * @deprecated ä½¿ç”¨ countTokens() ä»£æ›¿
   */
  public estimateTokens(text: string): number {
    // ç®€å•ä¼°ç®—ï¼šä¸­æ–‡ ~1.5 å­—ç¬¦/tokenï¼Œè‹±æ–‡ ~4 å­—ç¬¦/token
    // è¿™é‡Œä½¿ç”¨ä¿å®ˆä¼°è®¡ï¼š1 å­—ç¬¦ = 1 token
    return text.length;
  }

  /**
   * æ£€æŸ¥æ–‡æœ¬æ˜¯å¦è¶…è¿‡ä¸Šä¸‹æ–‡é™åˆ¶
   */
  public isWithinContextLimit(text: string): boolean {
    const estimatedTokens = this.estimateTokens(text);
    const maxTokens = this.getMaxContextTokens();
    return estimatedTokens < maxTokens * 0.8;  // ç•™ 20% ä½™é‡
  }

  /**
   * æˆªæ–­æ–‡æœ¬ä»¥é€‚åº”ä¸Šä¸‹æ–‡é™åˆ¶
   */
  public truncateToContextLimit(text: string, reservedTokens: number = 1000): string {
    const maxTokens = this.getMaxContextTokens() - this.getMaxOutputTokens() - reservedTokens;
    const estimatedTokens = this.estimateTokens(text);
    
    if (estimatedTokens <= maxTokens) {
      return text;
    }
    
    // ç®€å•æˆªæ–­ï¼ˆæŒ‰å­—ç¬¦æ•°ï¼‰
    const ratio = maxTokens / estimatedTokens;
    const truncatedLength = Math.floor(text.length * ratio);
    
    console.warn(`âš ï¸  æ–‡æœ¬è¿‡é•¿ï¼Œå·²æˆªæ–­: ${estimatedTokens} tokens â†’ ${maxTokens} tokens`);
    return text.substring(0, truncatedLength) + '\n\n[... æ–‡æœ¬å·²æˆªæ–­ ...]';
  }

  /**
   * è·å–æ¨¡å‹ä¿¡æ¯ï¼ˆä¾›å¤–éƒ¨æŸ¥è¯¢ï¼‰
   */
  public getModelInfoForDisplay(): string {
    if (!this.modelInfo) {
      return `æ¨¡å‹: ${this.config.model} (æœªçŸ¥é…ç½®)`;
    }
    
    return `æ¨¡å‹: ${this.modelInfo.name}
ä¸Šä¸‹æ–‡çª—å£: ${this.modelInfo.maxContextTokens.toLocaleString()} tokens
æœ€å¤§è¾“å‡º: ${this.modelInfo.maxOutputTokens.toLocaleString()} tokens
${this.modelInfo.costPer1kInputTokens ? `æˆæœ¬: $${this.modelInfo.costPer1kInputTokens}/1K è¾“å…¥, $${this.modelInfo.costPer1kOutputTokens}/1K è¾“å‡º` : ''}`;
  }
}

/**
 * ä»é…ç½®æ–‡ä»¶æˆ–ç¯å¢ƒå˜é‡åˆ›å»º LLM Provider
 */
export function createLLMProvider(): LLMProvider {
  // ä¼˜å…ˆä»ç¯å¢ƒå˜é‡è¯»å–
  const provider = (process.env.LLM_PROVIDER || 'siliconflow') as 'siliconflow' | 'openai' | 'custom';
  const apiKey = process.env.LLM_API_KEY || process.env.SILICONFLOW_API_KEY || '';
  const baseUrl = process.env.LLM_BASE_URL;
  const model = process.env.LLM_MODEL || PROVIDER_DEFAULTS[provider]?.defaultModel || 'Qwen/Qwen2.5-7B-Instruct';
  const maxTokens = process.env.LLM_MAX_TOKENS ? parseInt(process.env.LLM_MAX_TOKENS) : undefined;
  const temperature = process.env.LLM_TEMPERATURE ? parseFloat(process.env.LLM_TEMPERATURE) : undefined;
  
  if (!apiKey) {
    throw new Error('LLM API Key æœªè®¾ç½®ï¼Œè¯·è®¾ç½®ç¯å¢ƒå˜é‡ LLM_API_KEY æˆ– SILICONFLOW_API_KEY');
  }
  
  const config: LLMConfig = {
    provider,
    apiKey,
    baseUrl,
    model,
    maxTokens,
    temperature
  };
  
  const llm = new LLMProvider(config);
  
  console.log('âœ… LLM Provider å·²åˆå§‹åŒ–');
  console.log(`   Provider: ${provider}`);
  console.log(`   Model: ${model}`);
  console.log(`   Base URL: ${baseUrl || PROVIDER_DEFAULTS[provider]?.baseUrl || 'custom'}`);
  console.log(`   Max Output Tokens: ${llm.getMaxOutputTokens()}`);
  
  return llm;
}

